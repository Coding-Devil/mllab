import pandas as pd
import numpy as np

def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    return -np.sum([(counts[i] / np.sum(counts)) * np.log2(counts[i] / np.sum(counts)) for i in range(len(elements))])

def info_gain(data, split_attribute, target_attribute="PlayTennis"):
    total_entropy = entropy(data[target_attribute])
    vals, counts = np.unique(data[split_attribute], return_counts=True)
    weighted_entropy = np.sum([(counts[i] / np.sum(counts)) * entropy(data[data[split_attribute] == vals[i]][target_attribute]) 
                               for i in range(len(vals))])
    return total_entropy - weighted_entropy

def ID3(data, features, target_attribute="PlayTennis"):
    target_vals = np.unique(data[target_attribute])
    if len(target_vals) == 1:
        return target_vals[0]
    if not features:
        return data[target_attribute].mode()[0]
    
    best_feature = max(features, key=lambda f: info_gain(data, f, target_attribute))
    tree = {best_feature: {}}
    features = [f for f in features if f != best_feature]
    
    for value in np.unique(data[best_feature]):
        subtree_data = data[data[best_feature] == value]
        subtree = ID3(subtree_data, features, target_attribute)
        tree[best_feature][value] = subtree
        
    return tree

def classify(sample, tree):
    if not isinstance(tree, dict):
        return tree
    feature = list(tree.keys())[0]
    return classify(sample, tree[feature][sample[feature]])

df = pd.read_csv('enjoysportID3.csv')
features = list(df.columns[:-1])
tree = ID3(df, features)

from pprint import pprint
pprint(tree)

new_sample = {
    'Outlook': 'Sunny',
    'Temperature': 'Hot',
    'Humidity': 'High',
    'Wind': 'Strong',
}

classification = classify(new_sample, tree)
print(f"The new sample is classified as: {classification}")
